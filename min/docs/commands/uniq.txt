handle unique subsequent lines in a sorted file (reduce/pick)

EXAMPLES
	sort file.txt > uniq
		first sort lines, then reduce all duplicates to only 1 occurence, print to stdout
	uniq -i < sorted_lines.txt
		remove duplicates from sorted_lines.txt, case-insensitive, print to stdout
	sort file.txt | uniq -c | sort -n
		sort file.txt, count number of duplicates per group (output unique lines with a leading count), then sort numerically, print to stdout

FLAGS
	-u	output only lines that originally were unique
	-d	output only lines that originally were duplicates (one per group)
	-i	case insensitive matching
	-c	count occurences of each group of duplicate lines
